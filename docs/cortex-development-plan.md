# Cortex Development Plan
## A Local-First AI-Powered Second Brain

---

## Executive Summary

Cortex is a desktop application that acts as your personal knowledge management system. It captures content from anywhere, understands it deeply using AI, connects related ideas automatically, and surfaces relevant knowledge when you need it.

**What makes Cortex different:**
- **Local-First**: All data stays on your machine, AI runs locally by default
- **Privacy-First**: No accounts, no cloud sync, no data collection
- **Flexible AI**: Local models (Ollama) or cloud APIs (OpenAI, Anthropic, Google) â€” user's choice
- **Intelligent**: Semantic search, conversational retrieval, and automatic organization
- **Cross-Platform**: Runs on macOS, Windows, and Linux

---

## Part 1: Product Vision

### The Problem

Knowledge workers are drowning in information scattered across dozens of tools. You save an insightful article, highlight a key passage in a book, bookmark a useful tweetâ€”and then never find it again when you actually need it.

Current solutions fall short:
- **Note apps** (Notion, Obsidian) require manual organization and don't understand your content
- **Read-later apps** (Pocket, Instapaper) are graveyards of unread articles
- **AI assistants** (ChatGPT, Claude) have no memory of what you've learned
- **Cloud-based PKM** (Mem, Reflect) require trusting a third party with your most personal thoughts

### The Solution

Cortex acts as an external brain that:

1. **Captures** content from your browser, files, and quick notes with zero friction
2. **Processes** everything using local AI to extract meaning, not just keywords
3. **Connects** related ideas across sources and time automatically
4. **Retrieves** knowledge through natural conversation, not folder hierarchies

### Why Local-First?

The decision to build Cortex as a local-first application is foundational to the product:

| Concern | Cloud-Only Approach | Our Local-First Approach |
|---------|---------------------|--------------------------|
| **Privacy** | Your thoughts on someone else's server | Your data never leaves your device |
| **Ownership** | Company shuts down, data at risk | Single SQLite file you control forever |
| **Cost** | Ongoing subscription fees | One-time purchase, minimal ongoing costs |
| **Latency** | Network round-trips for every action | Instant, works offline |
| **Trust** | "We won't read your data" promises | Mathematically impossible to access |

For a "second brain" containing your private thoughts, research, and sensitive information, local-first isn't just a featureâ€”it's the only ethical choice.

**The Hybrid Option**: While data always stays local, users can optionally use cloud AI providers (OpenAI, Anthropic, Google) for inference. This is for users with limited hardware or those who want more capable models. Even then, only the text being processed is sent â€” your knowledge base, history, and connections remain on your machine.

---

## Part 2: Architecture Decisions

### Technology Stack Summary

| Layer | Technology | Why |
|-------|------------|-----|
| **Desktop Shell** | Tauri 2.0 (Rust) | Small binary, native performance, secure |
| **Frontend** | React + TypeScript + Vite | Fast dev, great ecosystem, type safety |
| **UI Components** | shadcn/ui + Tailwind | Beautiful defaults, rapid development |
| **Backend** | Python + FastAPI | AI ecosystem, LangGraph support |
| **Database** | SQLite + sqlite-vec | Single file, zero config, vector search built-in |
| **AI Orchestration** | LangGraph | Stateful workflows, retries, cycles |
| **Local AI** | Ollama | Easy model management, runs anywhere |
| **Cloud AI** | LiteLLM â†’ OpenAI/Anthropic/Google | Unified interface, direct API pricing |
| **Browser Extension** | Plasmo | Modern DX, cross-browser |

### Decision 1: Tauri + Python Sidecar

**What**: A Tauri desktop shell (Rust + React) that spawns a Python backend process.

**Why this hybrid approach:**

The obvious choice for a local AI app would be Electron + Node.js, but this has significant drawbacks:
- Electron apps are bloated (150MB+ just for the shell)
- Node.js lacks mature ML/AI libraries
- Python has the best ecosystem for LangChain, LangGraph, and AI tooling

Pure Rust would be ideal for performance, but:
- Rust has a steep learning curve
- The LangGraph ecosystem is Python-first
- Rapid iteration is harder in Rust

**Our solution** gets the best of both worlds:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Tauri Shell (Rust)          â”‚  â† Small, fast, native
â”‚  â€¢ Window management                â”‚
â”‚  â€¢ System tray                      â”‚
â”‚  â€¢ File system access               â”‚
â”‚  â€¢ IPC bridge to Python             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ localhost HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Python Sidecar (FastAPI)       â”‚  â† All the AI magic
â”‚  â€¢ LangGraph workflows              â”‚
â”‚  â€¢ SQLite database                  â”‚
â”‚  â€¢ Ollama integration               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Trade-offs accepted:**
- Two runtimes instead of one (acceptable: Python is usually already installed)
- Slightly more complex deployment (mitigated: we bundle Python with the app)
- IPC overhead (negligible: localhost HTTP is fast)

### Decision 2: SQLite + sqlite-vec for Storage

**What**: A single SQLite database file with the sqlite-vec extension for vector search.

**Why SQLite:**

For a local-first app, SQLite is the obvious choice:
- Zero configuration, no separate database server
- Battle-tested reliability (used in every iPhone, Android, browser)
- Single file that users can backup, move, or inspect
- Excellent performance for our scale (millions of chunks is fine)

**Why sqlite-vec over alternatives:**

| Option | Pros | Cons |
|--------|------|------|
| Pinecone/Weaviate | Powerful, scalable | Cloud-based, defeats local-first purpose |
| ChromaDB | Local, popular | Separate process, adds complexity |
| pgvector | Great for Postgres | Requires PostgreSQL server |
| **sqlite-vec** | Native SQLite extension, single file | Newer, smaller community |

sqlite-vec keeps everything in one fileâ€”your items, chunks, embeddings, and conversations all live together. This dramatically simplifies backup, sync, and portability.

**Data model overview:**

```
cortex.db
â”œâ”€â”€ items          â†’ Saved content (webpages, notes, PDFs)
â”œâ”€â”€ chunks         â†’ Semantic segments of each item
â”œâ”€â”€ embeddings     â†’ Vector representations (sqlite-vec virtual table)
â”œâ”€â”€ connections    â†’ Discovered relationships between items
â”œâ”€â”€ conversations  â†’ Chat history
â””â”€â”€ messages       â†’ Individual chat messages with citations
```

### Decision 3: Ollama + Direct APIs for AI

**What**: Ollama provides local AI inference by default. Direct cloud APIs (OpenAI, Anthropic, Google) are available as an alternative for users who need them.

**Why this hybrid approach:**

Local-first with Ollama:
- **Privacy maximum**: Nothing leaves the device
- **Works offline**: No internet required
- **Zero ongoing cost**: Just your electricity
- **User controls models**: Swap based on hardware and needs

But local models have real limitations:
- Require decent hardware (8GB+ RAM for usable models)
- Quality gap vs frontier models (GPT-4, Claude 3.5)
- Slower on CPU-only machines

**The solution**: Offer direct cloud APIs as a complement, not a replacement.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI Provider Layer                         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚       Ollama        â”‚      â”‚     Direct APIs         â”‚   â”‚
â”‚  â”‚      (Default)      â”‚      â”‚    (Alternative)        â”‚   â”‚
â”‚  â”‚                     â”‚      â”‚                         â”‚   â”‚
â”‚  â”‚  â€¢ Maximum privacy  â”‚      â”‚  â€¢ OpenAI               â”‚   â”‚
â”‚  â”‚  â€¢ Works offline    â”‚      â”‚  â€¢ Anthropic            â”‚   â”‚
â”‚  â”‚  â€¢ Free             â”‚      â”‚  â€¢ Google               â”‚   â”‚
â”‚  â”‚  â€¢ Needs 8GB+ RAM   â”‚      â”‚  â€¢ User provides keys   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                            â”‚                                 â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚                 â”‚  Unified Interface   â”‚                     â”‚
â”‚                 â”‚  embed() / chat()    â”‚                     â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                            â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                  LangGraph Workflows
```

**Why direct APIs over aggregators (OpenRouter, etc.):**

| Factor | Aggregators | Direct APIs |
|--------|-------------|-------------|
| **Reliability** | Middleman can fail | Only provider uptime matters |
| **Longevity** | Startup risk | OpenAI/Anthropic/Google aren't going anywhere |
| **Pricing** | 5-20% markup | Base price |
| **Complexity** | One API key | Multiple keys (but LiteLLM abstracts this) |

For a desktop app meant to last years, depending on a VC-funded aggregator is unnecessary risk. LiteLLM (open source, bundled locally) gives us unified API access without the middleman.

**Model recommendations:**

| Use Case | Ollama (Local) | Cloud Alternative |
|----------|---------------|-------------------|
| **Embeddings** | nomic-embed-text (274MB) | OpenAI text-embedding-3-small |
| **Chat (Budget)** | llama3.2:3b (2GB) | Google Gemini 1.5 Flash |
| **Chat (Quality)** | mistral:7b (4GB) | Anthropic Claude 3 Haiku |
| **Chat (Best)** | llama3.1:70b (40GB) | Claude 3.5 Sonnet / GPT-4o |

**Typical cloud costs** (for users who choose that path):
- Light usage (50 items/month): ~$0.10-0.25/month
- Heavy usage (200 items/month): ~$0.50-1.00/month
- Using premium models (Sonnet/GPT-4o): ~$2-5/month

### Decision 4: LangGraph for AI Workflows

**What**: LangGraph orchestrates multi-step AI operations as directed graphs with state management.

**Why we need workflow orchestration:**

A Second Brain has complex AI operations that aren't simple request-response:

1. **Processing a saved article:**
   - Parse HTML â†’ Chunk semantically â†’ Embed each chunk â†’ Extract metadata â†’ Validate quality â†’ Retry if poor â†’ Store results â†’ Discover connections

2. **Searching your knowledge:**
   - Analyze query intent â†’ Decompose if complex â†’ Vector search â†’ Full-text search â†’ Fuse results â†’ Evaluate relevance â†’ Expand query if poor â†’ Return results

3. **Chatting with your knowledge:**
   - Retrieve context â†’ Grade document relevance â†’ Rewrite query if needed â†’ Generate answer â†’ Check if grounded in sources â†’ Regenerate if hallucinating

These aren't single LLM callsâ€”they're stateful workflows with branching logic, retry loops, and quality gates.

**Why LangGraph specifically:**

| Alternative | Issue |
|-------------|-------|
| Plain functions | State management becomes spaghetti, no visibility |
| Celery/queue systems | Overkill for local, adds infrastructure |
| LangChain alone | Chains are linear, can't handle cycles |
| Custom state machine | Reinventing the wheel |

LangGraph provides:
- **Typed state**: Each workflow has a clear schema
- **Conditional edges**: Route to different nodes based on state
- **Cycles**: Naturally handle retry loops
- **Checkpointing**: Resume interrupted workflows (important for large imports)
- **Debuggability**: Visualize execution, trace issues

### Decision 5: React Frontend with TypeScript

**What**: The UI is a React application running in Tauri's webview.

**Why React:**

- Largest ecosystem of components (shadcn/ui gives us beautiful defaults)
- You likely already know it
- Excellent TypeScript support
- Works seamlessly with Tauri

**Key UI principles:**

1. **Speed over features**: The app must feel instant. No loading spinners for basic operations.
2. **Keyboard-first**: Power users live on the keyboard. Every action has a shortcut.
3. **Information density**: Show more, scroll less. This is a productivity tool, not a social app.
4. **Progressive disclosure**: Simple by default, powerful when needed.

---

### Decision 6: AI Provider Architecture

**What**: A unified provider interface that abstracts Ollama and direct cloud APIs behind a common API.

**Why this matters:**

Users have different needs:
- Privacy maximalists want everything local
- Users with weak hardware need cloud inference
- Power users want the best models regardless of where they run

The app shouldn't care which provider is used â€” LangGraph workflows call `embed()` and `chat()` without knowing if it's Ollama or Claude.

**Implementation approach:**

```python
# All providers implement this interface
class AIProvider:
    async def embed(text: str) -> list[float]
    async def embed_batch(texts: list[str]) -> list[list[float]]
    async def chat(messages: list, system: str) -> str
    async def stream_chat(messages: list) -> AsyncIterator[str]
```

**LiteLLM for cloud APIs:**

Rather than implementing separate clients for OpenAI, Anthropic, and Google, we use LiteLLM â€” an open-source library that provides a unified interface to 100+ LLM providers. It runs locally (bundled with the app), so there's no middleman service.

Benefits of LiteLLM:
- Single interface for all providers
- Automatic retries and fallbacks
- Cost tracking built-in
- User just provides their API keys

**User experience flow:**

```
First Launch
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  How would you like Cortex to process your content?         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ğŸ  Local AI (Recommended)                              â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  Everything runs on your machine using Ollama.          â”‚ â”‚
â”‚  â”‚  Maximum privacy â€” nothing leaves your computer.        â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  Requirements: 8GB RAM minimum, 16GB recommended        â”‚ â”‚
â”‚  â”‚  Cost: Free                                              â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚                                    [Set Up Ollama â†’]    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â˜ï¸ Cloud AI                                            â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  Use OpenAI, Anthropic, or Google for AI processing.   â”‚ â”‚
â”‚  â”‚  Better for older hardware or when you need top models. â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  Requirements: API key(s), internet connection          â”‚ â”‚
â”‚  â”‚  Cost: ~$0.10-1.00/month typical usage                  â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚                                   [Configure APIs â†’]    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ğŸ”€ Hybrid                                              â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  Use local AI by default, cloud for complex queries.   â”‚ â”‚
â”‚  â”‚  Best of both worlds.                                   â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚                                    [Set Up Both â†’]      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  â„¹ï¸ Your data always stays on your device.                  â”‚
â”‚    Only the text being processed is sent to cloud APIs.    â”‚
â”‚    You can change this anytime in Settings.                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Settings UI for cloud APIs:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Provider Settings                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Current Mode: â— Local (Ollama)  â—‹ Cloud  â—‹ Hybrid          â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Local AI (Ollama)                          Status: Running â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Embedding Model    [nomic-embed-text              â–¼]       â”‚
â”‚  Chat Model         [llama3.2:3b                   â–¼]       â”‚
â”‚                                                              â”‚
â”‚  [Download More Models]                                      â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Cloud APIs (Optional)                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                              â”‚
â”‚  OpenAI        [sk-proj-â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢]    âœ“ Connected      â”‚
â”‚  Anthropic     [Not configured]             [Add Key]        â”‚
â”‚  Google        [Not configured]             [Add Key]        â”‚
â”‚                                                              â”‚
â”‚  Cloud Model Preferences:                                    â”‚
â”‚  Embeddings     [openai/text-embedding-3-small     â–¼]       â”‚
â”‚  Chat           [anthropic/claude-3-haiku          â–¼]       â”‚
â”‚                                                              â”‚
â”‚  â˜‘ Enable fallback (if primary provider fails, try others)  â”‚
â”‚                                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Usage This Month                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Local:  1,247 queries                                       â”‚
â”‚  Cloud:  23 queries â”‚ ~$0.04 estimated                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Hybrid mode behavior:**

When hybrid mode is enabled:
1. **Embeddings**: Always use the configured embedding provider (usually local for speed)
2. **Simple extraction**: Use local model
3. **Complex chat queries**: User can choose per-query or set a default
4. **Fallback**: If local model fails or is unavailable, fall back to cloud

This gives users fine-grained control while keeping the default experience simple.

---

## Part 3: LangGraph Workflows

### Workflow 1: Content Processing

**Purpose**: Transform raw saved content into searchable, connected knowledge.

**Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  New Item   â”‚ User saves a webpage, uploads a PDF, or writes a note
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Classify   â”‚ Determine content type to route to appropriate parser
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”â”€â”€â”€â”€â”€â”€â”€â”
   â–¼       â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ HTML â”‚â”‚ PDF  â”‚â”‚Audio â”‚ Parse into clean text (different strategies per type)
â””â”€â”€â”¬â”€â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”€â”˜
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Chunk     â”‚ Split into semantic segments (not fixed-size)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ Preserve paragraph boundaries, add overlap
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Embed     â”‚ Generate vector embedding for each chunk
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ Batch process through Ollama
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Extract    â”‚ Use LLM to extract structured metadata:
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ - Summary (short and long)
       â”‚        - Key concepts
       â”‚        - Named entities (people, companies, products)
       â”‚        - Questions this content answers
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Validate   â”‚â”€â”€â”€â”€â”€â–¶â”‚    Retry     â”‚ If extraction quality is poor,
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ Poor â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ retry with different prompt
       â”‚ Good              â”‚
       â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Store     â”‚ Persist chunks, embeddings, and metadata
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Connect    â”‚ Find related items via embedding similarity
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Store bidirectional connections
```

**Why this design:**

- **Conditional parsing**: PDFs need OCR, audio needs transcription, HTML needs cleanup. One-size-fits-all fails.
- **Semantic chunking**: Fixed-size chunks (e.g., 500 tokens) break mid-sentence. Semantic chunking respects document structure.
- **Extraction validation**: LLMs sometimes produce garbage. A validation step catches obvious failures.
- **Async connection discovery**: Finding connections is slow but not urgent. Run it after the user gets their "saved!" confirmation.

### Workflow 2: Adaptive Search

**Purpose**: Find relevant content even when the user's query is vague or complex.

**Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Query     â”‚ "What do I know about pricing strategies for B2B SaaS?"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Analyze    â”‚ Classify query complexity:
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ - Simple: direct lookup ("machine learning basics")
       â”‚        - Multi-faceted: multiple aspects ("React vs Vue for large projects")
       â”‚        - Temporal: time-based ("articles from last month")
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Multi-faceted  â”‚
   â–¼                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚Decompose â”‚        â”‚ Break into sub-queries:
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚ - "B2B SaaS pricing models"
     â”‚              â”‚ - "pricing psychology"
     â”‚              â”‚ - "value-based pricing"
     â–¼              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—€â”€â”€â”€â”€â”€â”˜
â”‚Vector Searchâ”‚ Embed query, find similar chunks
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  FTS Search â”‚ Full-text search for exact phrases, names
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚    Fuse     â”‚ Reciprocal Rank Fusion combines both result sets
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ Items appearing in both rank higher
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Evaluate   â”‚â”€â”€â”€â”€â”€â–¶â”‚   Expand     â”‚ If results are poor,
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ Poor â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ add synonyms/related terms
       â”‚ Good              â”‚
       â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Return    â”‚ Ranked results with matched snippets
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this design:**

- **Hybrid search**: Vector search finds conceptually similar content. Full-text search finds exact matches. Neither alone is sufficient.
- **Query decomposition**: Complex queries often need multiple searches. "Compare X and Y" should search for both X and Y.
- **Automatic expansion**: If the first search returns nothing useful, try related terms before giving up.
- **RRF fusion**: A simple but effective algorithm for combining ranked lists. Items that appear in multiple searches are more likely relevant.

### Workflow 3: RAG Chat

**Purpose**: Answer questions using your personal knowledge base with citations.

**Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Message   â”‚ "What were the key insights from that pricing article?"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Retrieve   â”‚ Search for relevant chunks (using Search workflow)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Grade     â”‚ LLM evaluates each chunk: "Is this relevant to the question?"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ Filter out tangentially related content
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ No relevant docs      â”‚
   â–¼                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚ Rewrite  â”‚               â”‚ Transform query for better retrieval:
â”‚  Query   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ "pricing article insights" â†’ 
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  Retry search â”‚ "pricing strategy key points summary"
     â”‚                     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Have relevant docs
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Generate   â”‚ Build prompt with context, generate answer
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ Include citations: "According to [Article Title]..."
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Ground    â”‚ Verify answer is supported by sources
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ Catch hallucinations before showing user
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Not grounded          â”‚
   â–¼                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚Regenerateâ”‚               â”‚ Try again with stricter prompt
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚
     â”‚                     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Grounded
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Return    â”‚ Answer with citations to source items
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this design:**

- **Document grading**: Not all retrieved content is useful. Filtering before generation improves answer quality.
- **Query rewriting**: The user's natural question often isn't the best search query. Reformulating helps retrieval.
- **Grounding check**: Local LLMs hallucinate. A second pass catches answers that aren't supported by sources.
- **Citations**: Users need to verify AI answers. Always link back to the original content.

### Workflow 4: Connection Discovery

**Purpose**: Automatically find relationships between items in your knowledge base.

**Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Item Ready  â”‚ Triggered after processing completes
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚Find Similar â”‚ Vector search for items with similar embeddings
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ 
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Extract    â”‚ Get entities (people, companies, concepts) from this item
â”‚  Entities   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Match     â”‚ Find other items mentioning the same entities
â”‚  Entities   â”‚ "This article mentions Stripe, you have 5 others about Stripe"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Temporal   â”‚ Find items saved around the same time
â”‚  Cluster    â”‚ "You saved these 4 items the same weekâ€”likely related research"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Score     â”‚ Assign connection strength (0-1) based on evidence
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ Multiple signals (similar + same entity) = stronger connection
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Store     â”‚ Save bidirectional connections above threshold
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this design:**

- **Multiple signals**: Similarity alone misses connections. Entity matching catches "different articles about the same company."
- **Temporal clustering**: What you saved together is often related. This captures research sessions.
- **Strength scoring**: Not all connections are equal. "Same author" is weaker than "same topic + same entities."
- **Background processing**: This runs after the user has moved on. Don't block the save confirmation.

### Workflow 5: Daily Digest

**Purpose**: Proactively surface insights and forgotten gems.

**Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scheduled  â”‚ Runs daily (or weekly, user preference)
â”‚   Trigger   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Gather    â”‚ Recent items (last 7 days)
â”‚   Recent    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Find New   â”‚ Connections discovered since last digest
â”‚ Connections â”‚ "Your article on X relates to your note on Y"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Surface    â”‚ High-value old items you haven't accessed
â”‚   Gems      â”‚ "6 months ago you saved thisâ€”still relevant?"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Generate   â”‚ LLM creates insights across items:
â”‚  Insights   â”‚ "You've been researching pricing a lot. Key themes are..."
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Compose    â”‚ Format into readable digest
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Notify    â”‚ Show in app, optionally send to system notifications
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this design:**

- **Proactive value**: Users save content and forget it. The digest brings knowledge back.
- **Serendipity**: Surfacing old content creates unexpected connections.
- **Synthesis**: The LLM can see patterns across items that humans miss.
- **Respecting attention**: Daily is optional. Some users want weekly or manual-only.

---

## Part 4: Development Phases

### Phase 1: Foundation (Weeks 1-4)

**Goal**: A working app that can save and retrieve content with flexible AI backend.

**Deliverables:**
1. Tauri app shell with React frontend
2. Python sidecar with FastAPI
3. SQLite database with schema
4. AI Provider abstraction layer:
   - Ollama provider implementation
   - Direct API provider via LiteLLM (OpenAI, Anthropic, Google)
   - Unified interface for LangGraph workflows
5. Basic item CRUD (create, read, update, delete)
6. Provider selection onboarding flow
7. Simple item list UI

**Technical milestones:**
- [ ] Tauri spawns Python process on startup, manages lifecycle
- [ ] Frontend can call Python endpoints via localhost
- [ ] Items persist in SQLite and survive app restart
- [ ] Ollama provider works with health check and model verification
- [ ] LiteLLM provider works with at least one cloud API (OpenAI)
- [ ] User can switch providers in settings

**Why start here:**
The AI provider layer is foundational â€” every subsequent feature depends on it. Getting this right early prevents painful refactors later. Users should be able to choose their provider from day one.

### Phase 2: Processing Pipeline (Weeks 5-8)

**Goal**: Saved content is processed and searchable.

**Deliverables:**
1. Processing Graph (LangGraph) with:
   - HTML parsing and cleanup
   - Semantic chunking
   - Embedding generation via Ollama
   - Basic metadata extraction
2. Vector search via sqlite-vec
3. Full-text search via SQLite FTS5
4. Processing status UI (pending â†’ processing â†’ ready)

**Technical milestones:**
- [ ] Saving a webpage triggers processing automatically
- [ ] Embeddings stored in sqlite-vec and searchable
- [ ] Search returns relevant chunks with snippets
- [ ] Failed processing can be retried

**Why this phase:**
Search is the core value proposition. Without good retrieval, chat and connections don't work. This phase proves the AI pipeline functions.

### Phase 3: Browser Extension (Weeks 9-10)

**Goal**: Users can capture content from their browser with one click.

**Deliverables:**
1. Chrome extension (Firefox later)
   - Popup with "Save to Cortex" button
   - Content extraction from current page
   - Highlight capture
2. Local HTTP endpoint for extension communication
3. Keyboard shortcut support

**Technical milestones:**
- [ ] Extension detects running Cortex app
- [ ] One-click save extracts title, URL, content
- [ ] Highlighted text saved with context
- [ ] Works on major sites (articles, Twitter, YouTube)

**Why this phase:**
Capture friction kills adoption. If saving is hard, users won't build a knowledge base. The extension makes capture effortless.

### Phase 4: RAG Chat (Weeks 11-14)

**Goal**: Users can have conversations with their knowledge base.

**Deliverables:**
1. Chat Graph (LangGraph) with:
   - Context retrieval
   - Document grading
   - Query rewriting
   - Grounded generation
2. Chat UI with:
   - Conversation history
   - Citations linking to source items
   - Streaming responses
3. Conversation persistence

**Technical milestones:**
- [ ] Chat answers questions using saved content
- [ ] Citations are clickable and accurate
- [ ] Conversation history persists across sessions
- [ ] Response quality is acceptable with local models

**Why this phase:**
Chat is the primary interface for knowledge retrieval. It's more natural than search for many queries and demonstrates AI value clearly.

### Phase 5: Connections & Intelligence (Weeks 15-18)

**Goal**: The app proactively surfaces insights and connections.

**Deliverables:**
1. Connection Discovery Graph
2. Adaptive Search Graph with:
   - Query decomposition
   - Automatic expansion
3. Metadata extraction improvements:
   - Entity extraction
   - Concept tagging
4. "Related Items" sidebar in item view
5. Daily Digest (optional, user-enabled)

**Technical milestones:**
- [ ] Items show related content automatically
- [ ] Search handles complex queries gracefully
- [ ] Daily digest surfaces forgotten content
- [ ] Entity-based connections work (same person/company)

**Why this phase:**
Connections transform a "save and search" tool into a true Second Brain. This is where Cortex becomes meaningfully different from alternatives.

### Phase 6: Polish & Launch (Weeks 19-22)

**Goal**: Production-ready application.

**Deliverables:**
1. Onboarding flow with:
   - AI provider selection (Ollama vs Cloud vs Hybrid)
   - Ollama setup wizard with model downloads
   - API key configuration for cloud providers
2. Settings UI:
   - Model selection per task
   - Provider switching
   - Usage tracking and cost estimates
3. Import from existing tools (Pocket, Readwise export)
4. Export (Markdown, JSON backup)
5. Auto-updates via Tauri
6. Crash reporting and error handling
7. Performance optimization
8. Platform-specific builds (macOS, Windows, Linux)

**Technical milestones:**
- [ ] New user can go from download to first save in < 5 minutes
- [ ] Provider switching works without data loss
- [ ] App handles 10,000+ items without performance degradation
- [ ] Crash reports collected (locally) for debugging
- [ ] Signed builds for all platforms

**Why this phase:**
Polish separates a prototype from a product. Onboarding, updates, and error handling are invisible when done right but fatal when missing.

---

## Part 5: Technical Risks & Mitigations

### Risk 1: Local LLM Quality

**Concern**: Local models may produce poor extractions or answers compared to GPT-4/Claude.

**Mitigations:**
- Validation loops in workflows catch obvious failures
- Extraction prompts optimized for smaller models (simpler, more explicit)
- Users with powerful hardware can use larger local models (7B+)
- **Cloud API option**: Users who need quality can use direct APIs (Anthropic, OpenAI, Google)
- Clear guidance on which tasks benefit most from better models

### Risk 2: Ollama Dependency

**Concern**: Requiring users to install Ollama adds friction.

**Mitigations:**
- Clear onboarding wizard with one-click Ollama install
- App detects missing Ollama and guides user through setup
- Future option: bundle llama.cpp directly (no external dependency)

### Risk 3: sqlite-vec Maturity

**Concern**: sqlite-vec is newer than alternatives like ChromaDB.

**Mitigations:**
- Extensive testing with realistic data volumes
- Fallback plan: migrate to ChromaDB if issues arise (similar API)
- sqlite-vec is actively maintained and used in production by others

### Risk 4: Cross-Platform Python Bundling

**Concern**: Shipping Python with a desktop app is complex.

**Mitigations:**
- PyInstaller or PyOxidizer creates standalone executable
- Tauri's sidecar feature handles process management
- Test extensively on all platforms before each release

### Risk 5: Processing Performance

**Concern**: Embedding many chunks is slow with local models.

**Mitigations:**
- Batch embedding reduces Ollama overhead
- Background processing doesn't block UI
- Progress indicators keep users informed
- "Process later" option for bulk imports

---

## Part 6: Future Roadmap (Post-Launch)

### Tier 1: High Priority Additions

1. **PDF Support**: Parse and chunk PDF documents, OCR for scanned pages
2. **Audio/Video**: Transcribe podcasts and videos via Whisper
3. **Mobile Companion**: React Native app for capture on the go (syncs via local network)
4. **More Browsers**: Firefox and Safari extensions

### Tier 2: Differentiating Features

1. **Writing Assistant**: Surface relevant knowledge while you write (integrates with editors)
2. **Smart Collections**: Auto-updating folders based on search queries
3. **Graph Visualization**: See connections between items visually
4. **Bi-directional Sync**: Optional integration with Notion, Obsidian (user-controlled)

### Tier 3: Expansion

1. **Team Features**: Share collections with teammates (still local-first, encrypted sync)
2. **Plugins API**: Let users extend capture and processing
3. **Custom Models**: Support for fine-tuned models on user's data (fully local)

---

## Part 7: Success Metrics

### User Success

| Metric | Target | Why It Matters |
|--------|--------|----------------|
| Items captured per week | 10+ | Measures habit formation |
| Search queries per week | 5+ | Proves retrieval value |
| Chat sessions per week | 3+ | Shows AI is useful |
| 30-day retention | 40%+ | Real stickiness |

### Technical Health

| Metric | Target | Why It Matters |
|--------|--------|----------------|
| Processing success rate | 95%+ | Pipeline reliability |
| Search latency (p95) | < 500ms | Feels instant |
| Chat response time | < 5s | Acceptable for local LLM |
| App crash rate | < 1% | Stability |

### Business (If Monetized)

| Metric | Target | Why It Matters |
|--------|--------|----------------|
| Downloads | 10K in 6 months | Market validation |
| Paid conversions | 5%+ | Sustainable business |
| Refund rate | < 5% | Product satisfaction |

---

## Conclusion

Cortex is a bet on three trends:

1. **Privacy backlash**: Users increasingly want their data local, not in the cloud
2. **Local AI maturity**: Open-source models are finally good enough for real applications
3. **Knowledge overload**: The problem of fragmented information is getting worse, not better

By combining a local-first architecture with LangGraph-orchestrated AI workflows and flexible provider options, we can build a Second Brain that's private by default, intelligent, and accessible to users regardless of their hardware.

**The key insight**: Data locality and AI capability don't have to be at odds. Your knowledge base stays on your machine always. Only the AI inference is flexible â€” run it locally for maximum privacy, or use cloud APIs when you need more power. Users choose their own trade-off.

The path from here to launch is clear:
1. Build the foundation (Tauri + Python + SQLite + Provider Layer)
2. Prove the AI works (Processing + Search)
3. Make capture effortless (Browser Extension)
4. Deliver the magic (Chat + Connections)
5. Polish and ship

Let's build the knowledge tool we wish existed.

---

*Development Plan Version: 2.0 (Ollama + Direct APIs)*
*Last Updated: December 2024*
